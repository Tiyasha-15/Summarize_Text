{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\"\"\n",
    "Natural language processing (NLP) is a subfield of computer science and artificial intelligence (AI) that uses machine learning to enable computers to understand and communicate with human language.NLP enables computers and digital devices to recognize, understand and generate text and speech by combining computational linguistics—the rule-based modeling of human language—together with statistical modeling, machine learning (ML) and deep learning.NLP research has enabled the era of generative AI, from the communication skills of large language models (LLMs) to the ability of image generation models to understand requests. NLP is already part of everyday life for many, powering search engines, prompting chatbots for customer service with spoken commands, voice-operated GPS systems and digital assistants on smartphones.NLP also plays a growing role in enterprise solutions that help streamline and automate business operations, increase employee productivity and simplify mission-critical business processes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "stemming = PorterStemmer()\n",
    "lemmatize = WordNetLemmatizer()\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)  \n",
    "    filtered_words = []\n",
    "    for word in words:\n",
    "        word = re.sub(r\"[,.()]\", \" \", word)\n",
    "        word = re.sub(\"[^a-zA-Z]\", \"\", word)\n",
    "        word = lemmatize.lemmatize(word)\n",
    "        word = word.lower()\n",
    "        if word and word not in stopwords.words('english'):\n",
    "            filtered_words.append(word)\n",
    "    # Add the filtered words of the sentence to the corpus\n",
    "    if filtered_words:\n",
    "        corpus.append(filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 22:17:07.727 INFO    gensim.models.word2vec: collecting all words and their counts\n",
      "2024-08-13 22:17:07.728 INFO    gensim.models.word2vec: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-08-13 22:17:07.728 INFO    gensim.models.word2vec: collected 85 word types from a corpus of 99 raw words and 2 sentences\n",
      "2024-08-13 22:17:07.729 INFO    gensim.models.word2vec: Creating a fresh vocabulary\n",
      "2024-08-13 22:17:07.729 INFO    gensim.utils: Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 85 unique words (100.00% of original 85, drops 0)', 'datetime': '2024-08-13T22:17:07.729670', 'gensim': '4.3.3', 'python': '3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2024-08-13 22:17:07.730 INFO    gensim.utils: Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 99 word corpus (100.00% of original 99, drops 0)', 'datetime': '2024-08-13T22:17:07.730668', 'gensim': '4.3.3', 'python': '3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2024-08-13 22:17:07.731 INFO    gensim.models.word2vec: deleting the raw counts dictionary of 85 items\n",
      "2024-08-13 22:17:07.732 INFO    gensim.models.word2vec: sample=0.001 downsamples 85 most-common words\n",
      "2024-08-13 22:17:07.732 INFO    gensim.utils: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 36.92358699465882 word corpus (37.3%% of prior 99)', 'datetime': '2024-08-13T22:17:07.732660', 'gensim': '4.3.3', 'python': '3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2024-08-13 22:17:07.733 INFO    gensim.models.word2vec: estimated required memory for 85 words and 100 dimensions: 110500 bytes\n",
      "2024-08-13 22:17:07.734 INFO    gensim.models.word2vec: resetting layer weights\n",
      "2024-08-13 22:17:07.734 INFO    gensim.utils: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-08-13T22:17:07.734657', 'gensim': '4.3.3', 'python': '3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n",
      "2024-08-13 22:17:07.735 INFO    gensim.utils: Word2Vec lifecycle event {'msg': 'training model with 4 workers on 85 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-08-13T22:17:07.735652', 'gensim': '4.3.3', 'python': '3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "2024-08-13 22:17:07.738 INFO    gensim.models.word2vec: EPOCH 0: training on 99 raw words (38 effective words) took 0.0s, 97938 effective words/s\n",
      "2024-08-13 22:17:07.740 INFO    gensim.models.word2vec: EPOCH 1: training on 99 raw words (37 effective words) took 0.0s, 102465 effective words/s\n",
      "2024-08-13 22:17:07.742 INFO    gensim.models.word2vec: EPOCH 2: training on 99 raw words (49 effective words) took 0.0s, 139920 effective words/s\n",
      "2024-08-13 22:17:07.744 INFO    gensim.models.word2vec: EPOCH 3: training on 99 raw words (37 effective words) took 0.0s, 103064 effective words/s\n",
      "2024-08-13 22:17:07.746 INFO    gensim.models.word2vec: EPOCH 4: training on 99 raw words (36 effective words) took 0.0s, 107335 effective words/s\n",
      "2024-08-13 22:17:07.747 INFO    gensim.utils: Word2Vec lifecycle event {'msg': 'training on 495 raw words (197 effective words) took 0.0s, 17028 effective words/s', 'datetime': '2024-08-13T22:17:07.747620', 'gensim': '4.3.3', 'python': '3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "2024-08-13 22:17:07.747 INFO    gensim.utils: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=85, vector_size=100, alpha=0.025>', 'datetime': '2024-08-13T22:17:07.747620', 'gensim': '4.3.3', 'python': '3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(corpus, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['understand',\n",
       " 'computer',\n",
       " 'digital',\n",
       " 'ai',\n",
       " 'modeling',\n",
       " 'human',\n",
       " 'learning',\n",
       " 'machine',\n",
       " 'model',\n",
       " 'business',\n",
       " 'language',\n",
       " 'nlp',\n",
       " 'enabled',\n",
       " 'computational',\n",
       " 'linguisticsthe',\n",
       " 'rulebased',\n",
       " 'languagetogether',\n",
       " 'statistical',\n",
       " 'ml',\n",
       " 'ha',\n",
       " 'deep',\n",
       " 'era',\n",
       " 'speech',\n",
       " 'learningnlp',\n",
       " 'research',\n",
       " 'combining',\n",
       " 'process',\n",
       " 'text',\n",
       " 'generate',\n",
       " 'recognize',\n",
       " 'device',\n",
       " 'communication',\n",
       " 'enables',\n",
       " 'languagenlp',\n",
       " 'communicate',\n",
       " 'enable',\n",
       " 'us',\n",
       " 'intelligence',\n",
       " 'artificial',\n",
       " 'science',\n",
       " 'subfield',\n",
       " 'processing',\n",
       " 'generative',\n",
       " 'large',\n",
       " 'skill',\n",
       " 'voiceoperated',\n",
       " 'system',\n",
       " 'assistant',\n",
       " 'smartphonesnlp',\n",
       " 'also',\n",
       " 'play',\n",
       " 'growing',\n",
       " 'role',\n",
       " 'enterprise',\n",
       " 'solution',\n",
       " 'help',\n",
       " 'streamline',\n",
       " 'automate',\n",
       " 'operation',\n",
       " 'increase',\n",
       " 'employee',\n",
       " 'productivity',\n",
       " 'simplify',\n",
       " 'gps',\n",
       " 'command',\n",
       " 'missioncritical',\n",
       " 'spoken',\n",
       " 'llms',\n",
       " 'ability',\n",
       " 'image',\n",
       " 'generation',\n",
       " 'request',\n",
       " 'already',\n",
       " 'part',\n",
       " 'everyday',\n",
       " 'life',\n",
       " 'many',\n",
       " 'powering',\n",
       " 'search',\n",
       " 'engine',\n",
       " 'prompting',\n",
       " 'chatbots',\n",
       " 'customer',\n",
       " 'service',\n",
       " 'natural']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-2.8959188e-05,  4.7350611e-04,  6.4723019e-04,  1.9013547e-04,\n",
       "        -2.7775078e-04, -1.3800929e-03,  1.0569778e-03,  1.4916268e-03,\n",
       "        -1.0229605e-03, -1.4249383e-03,  6.7169784e-04, -1.5043070e-03,\n",
       "        -3.3956437e-04,  6.4421369e-04,  4.6630049e-04, -2.8897010e-04,\n",
       "         1.4717258e-03,  2.3834567e-04, -5.5088819e-04, -2.3022473e-03,\n",
       "         2.0914518e-04,  6.6685065e-04,  1.4642999e-03, -5.3536781e-04,\n",
       "        -2.0381941e-04,  8.9065533e-04, -5.0788769e-04,  4.2513173e-04,\n",
       "        -7.1755279e-04,  6.0918741e-04,  1.1077836e-03, -9.1778173e-04,\n",
       "         1.0007798e-03, -2.8632958e-03, -6.9332232e-06,  9.3185349e-04,\n",
       "         1.1620580e-03, -5.5402680e-04, -4.6247084e-04, -7.7671767e-04,\n",
       "         1.8634855e-04, -1.4470353e-03, -7.3379301e-04,  2.0319106e-04,\n",
       "         1.1749469e-03, -9.4917422e-04, -8.1605773e-04, -5.8202480e-04,\n",
       "         5.6263048e-04,  1.9837706e-04,  3.5234602e-04, -5.5839756e-04,\n",
       "        -3.2755287e-04,  9.7095457e-05, -5.1894702e-04,  1.4966702e-04,\n",
       "         1.0441461e-03, -2.0117145e-04, -7.5783741e-05,  2.4379308e-04,\n",
       "        -7.1924651e-04, -7.8220241e-04,  1.1514712e-03,  5.8230228e-04,\n",
       "        -8.3017180e-04,  2.3075712e-03, -6.5726636e-04,  1.2991946e-03,\n",
       "        -1.6148138e-03,  7.4678869e-04,  6.6037191e-04,  9.8985469e-04,\n",
       "         1.5214811e-03,  1.3950622e-03,  1.3416593e-03,  2.6172980e-05,\n",
       "        -4.1451113e-04,  8.3958200e-04, -3.4424925e-04, -6.7487068e-04,\n",
       "        -1.8341916e-03, -3.9226571e-04,  3.3347055e-04,  7.0091977e-04,\n",
       "        -2.2203179e-05, -3.6566763e-04,  1.4457268e-03,  1.8411191e-04,\n",
       "         5.1196822e-04,  7.0656615e-04,  1.2328263e-03,  3.9851031e-04,\n",
       "         2.6279880e-04, -8.2142890e-04,  1.5898779e-03,  6.5922056e-04,\n",
       "         3.3836073e-04, -1.3245675e-03, -2.5538128e-04, -1.4103338e-04],\n",
       "       dtype=float32),\n",
       " array([-4.59248346e-04,  1.24743767e-03, -1.09102181e-03,  1.97588990e-04,\n",
       "         1.74241187e-03, -1.82250235e-03,  1.00513324e-04,  2.76325201e-03,\n",
       "        -8.89282033e-04, -6.79279969e-04, -1.24327512e-03, -1.09330995e-03,\n",
       "         2.04119831e-04,  6.77119417e-04,  1.72549253e-03, -6.80057099e-04,\n",
       "        -1.67711952e-03, -1.11354806e-03, -8.96785423e-05, -1.44970859e-03,\n",
       "         1.05146191e-03,  1.00824145e-04,  1.67949929e-05, -4.62543976e-04,\n",
       "        -4.95859713e-04, -1.55691709e-03, -1.29163079e-03, -7.58616661e-04,\n",
       "        -3.07422451e-04, -1.19670127e-04, -2.79162145e-06,  2.14337153e-04,\n",
       "         1.39486976e-04,  3.59891856e-04, -7.69774488e-04,  1.44951080e-03,\n",
       "        -1.63861245e-04,  3.59022320e-04,  3.01170832e-04, -1.64630322e-03,\n",
       "        -3.12315433e-06,  9.70430090e-04,  2.27207391e-04, -2.81512330e-04,\n",
       "        -1.16379648e-04,  8.27735406e-04, -6.97006297e-04,  2.09015241e-04,\n",
       "         1.19641109e-03,  9.56547854e-04,  1.96697802e-04, -8.59668071e-04,\n",
       "         4.06692998e-04, -6.02531072e-04,  9.38842306e-04, -7.69092410e-04,\n",
       "        -2.53582170e-04, -6.39368256e-04, -9.51267895e-04, -1.67403938e-04,\n",
       "         1.66266260e-03,  9.79125616e-04, -2.69637647e-04, -1.34679861e-03,\n",
       "        -8.81958753e-04,  8.58253552e-05,  1.10204646e-03,  5.81077416e-04,\n",
       "        -1.93703652e-03,  1.22718967e-03, -1.54375448e-03,  2.03199568e-04,\n",
       "         8.49736898e-05, -1.10008533e-03,  7.76137575e-04,  9.12832693e-05,\n",
       "        -8.86794624e-06, -3.91905487e-04, -7.39749405e-04, -5.54902770e-04,\n",
       "         1.06971199e-03,  3.84830229e-04, -3.91974021e-03,  8.01671180e-04,\n",
       "        -1.31888292e-03, -1.04799855e-03, -7.38865347e-05,  3.21325089e-04,\n",
       "         1.76333007e-03,  1.00568880e-03,  1.64396374e-03, -1.34252434e-04,\n",
       "        -2.80284381e-04,  1.68738898e-03,  1.67993573e-03,  3.80405283e-04,\n",
       "        -4.81526193e-04,  9.33550415e-04,  5.72867517e-04, -1.40017061e-03],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv = []\n",
    "for sentence in corpus:\n",
    "    wv = [model.wv[word] for word in sentence if word in model.wv]\n",
    "    \n",
    "    if wv:  \n",
    "        sentence_vector = np.mean(wv, axis=0)  \n",
    "    else:\n",
    "        sentence_vector = np.zeros(model.vector_size) \n",
    "    \n",
    "    sv.append(sentence_vector)\n",
    "sv   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.4410376e-04,  8.6047186e-04, -2.2189581e-04,  1.9386223e-04,\n",
       "        7.3233055e-04, -1.6012976e-03,  5.7874556e-04,  2.1274395e-03,\n",
       "       -9.5612125e-04, -1.0521092e-03, -2.8578864e-04, -1.2988085e-03,\n",
       "       -6.7722271e-05,  6.6066656e-04,  1.0958965e-03, -4.8451358e-04,\n",
       "       -1.0269688e-04, -4.3760118e-04, -3.2028338e-04, -1.8759780e-03,\n",
       "        6.3030352e-04,  3.8383741e-04,  7.4054743e-04, -4.9895589e-04,\n",
       "       -3.4983957e-04, -3.3313088e-04, -8.9975924e-04, -1.6674247e-04,\n",
       "       -5.1248760e-04,  2.4475864e-04,  5.5249600e-04, -3.5172229e-04,\n",
       "        5.7013339e-04, -1.2517020e-03, -3.8835386e-04,  1.1906822e-03,\n",
       "        4.9909839e-04, -9.7502241e-05, -8.0650003e-05, -1.2115105e-03,\n",
       "        9.1612696e-05, -2.3830263e-04, -2.5329282e-04, -3.9160637e-05,\n",
       "        5.2928360e-04, -6.0719409e-05, -7.5653201e-04, -1.8650478e-04,\n",
       "        8.7952078e-04,  5.7746249e-04,  2.7452191e-04, -7.0903281e-04,\n",
       "        3.9570063e-05, -2.5271781e-04,  2.0994764e-04, -3.0971269e-04,\n",
       "        3.9528200e-04, -4.2026985e-04, -5.1352580e-04,  3.8194572e-05,\n",
       "        4.7170804e-04,  9.8461605e-05,  4.4091674e-04, -3.8224817e-04,\n",
       "       -8.5606531e-04,  1.1966983e-03,  2.2239005e-04,  9.4013603e-04,\n",
       "       -1.7759252e-03,  9.8698912e-04, -4.4169129e-04,  5.9652713e-04,\n",
       "        8.0322736e-04,  1.4748843e-04,  1.0588984e-03,  5.8728125e-05,\n",
       "       -2.1168953e-04,  2.2383826e-04, -5.4199935e-04, -6.1488675e-04,\n",
       "       -3.8223981e-04, -3.7177379e-06, -1.7931348e-03,  7.5129548e-04,\n",
       "       -6.7054306e-04, -7.0683309e-04,  6.8592012e-04,  2.5271851e-04,\n",
       "        1.1376492e-03,  8.5612747e-04,  1.4383950e-03,  1.3212895e-04,\n",
       "       -8.7427907e-06,  4.3298004e-04,  1.6349067e-03,  5.1981292e-04,\n",
       "       -7.1582734e-05, -1.9550853e-04,  1.5874312e-04, -7.7060203e-04],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv = np.mean(sv, axis=0)\n",
    "dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('natural language processing nlp subfield computer science artificial intelligence ai us machine learning enable computer understand communicate human languagenlp enables computer digital device recognize understand generate text speech combining computational linguisticsthe rulebased modeling human languagetogether statistical modeling machine learning ml deep learningnlp research ha enabled era generative ai communication skill large language model llms ability image generation model understand request',\n",
       "  4.919895e-05),\n",
       " ('nlp already part everyday life many powering search engine prompting chatbots customer service spoken command voiceoperated gps system digital assistant smartphonesnlp also play growing role enterprise solution help streamline automate business operation increase employee productivity simplify missioncritical business process',\n",
       "  5.9663125e-05)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_score = []\n",
    "for i, sv in enumerate(sv):\n",
    "    score = np.dot(sv, dv)  \n",
    "    rs = \" \".join(corpus[i])\n",
    "    sentence_score.append((rs, score))\n",
    "\n",
    "sentence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_score.sort(key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_length = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \" \".join([sentence for sentence, score in sentence_score[:summary_length]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nlp already part everyday life many powering search engine prompting chatbots customer service spoken command voiceoperated gps system digital assistant smartphonesnlp also play growing role enterprise solution help streamline automate business operation increase employee productivity simplify missioncritical business process'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
